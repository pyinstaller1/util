



import fitz  # PyMuPDF

def get_pdf(path):
    doc = fitz.open(path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text




def chunk_text(text, chunk_size=1000, overlap=200):
    chunks = []
    start = 0
    text_length = len(text)

    while start < text_length:
        end = start + chunk_size
        if end > text_length:
            end = text_length
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size - overlap  # ê²¹ì¹˜ëŠ” ë¶€ë¶„ ìœ ì§€í•˜ë©° ì´ë™

    return chunks





text = ''
# text += get_pdf(".\ê±´ê°•ë³´í—˜ìë£Œ.pdf")
# chunks = chunk_text(text, chunk_size=1000, overlap=200)


'''
print(f"ì´ ì²­í¬ ê°œìˆ˜: {len(chunks)}")
print(chunks[0])   # ì²« ë²ˆì§¸ ì²­í¬ ë‚´ìš© ì¶œë ¥
print(len(chunks[0]))
'''























from transformers import AutoTokenizer, AutoModel
import torch

# ëª¨ë¸ ë¡œë“œ (ì˜¤í”„ë¼ì¸ ê²½ë¡œë¼ë©´ í•´ë‹¹ ë””ë ‰í† ë¦¬ë¡œ ë°”ê¾¸ì„¸ìš”)
model_path = "./ko-sroberta-multitask"  # ë¡œì»¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModel.from_pretrained(model_path)

# ì…ë ¥ ë¬¸ì¥
text = "ë‚˜ëŠ” í•™ìƒì´ë‹¤"

# í† í¬ë‚˜ì´ì¦ˆ ë° í…ì„œ ë³€í™˜
inputs = tokenizer(text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

# ëª¨ë“  í† í°ì˜ ì„ë² ë”© ë²¡í„° ì¶”ì¶œ: shape = (1, í† í°ìˆ˜, 768)
token_embeddings = outputs.last_hidden_state[0]  # shape: (í† í°ìˆ˜, 768)

# í† í° í™•ì¸
tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

# ê²°ê³¼ ì¶œë ¥
for i, (token, vec) in enumerate(zip(tokens, token_embeddings)):
    print(f"\nğŸŸ¢ Token {i}: '{token}'")
    print(f"ğŸ”¢ Vector (768-dim):\n{vec.tolist()[:10]} ...")  # ì²˜ìŒ 10ê°œë§Œ ë³´ê¸° ì¢‹ê²Œ





















