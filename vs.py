import streamlit as st
import subprocess
import re
from transformers import MarianMTModel, MarianTokenizer, AutoTokenizer, AutoModel
import torch

import faiss, json
import numpy as np
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)









# -----------------------------
# RAG: FAISS + JSON Î°úÏª¨
index = faiss.read_index("vector_index.faiss")
with open("chunks.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

tokenizer_embed = AutoTokenizer.from_pretrained("./ko-sroberta-multitask")
model_embed = AutoModel.from_pretrained("./ko-sroberta-multitask")
model_embed.eval()

def embed_query(query):
    inputs = tokenizer_embed(query, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model_embed(**inputs)
    return outputs.last_hidden_state[:,0,:].squeeze().numpy()

def retrieve_context(query, top_k=1):
    print(query)
    query_vec = embed_query(query)
    D, I = index.search(np.array([query_vec]), k=top_k)
    print(777)

    for rank, idx in enumerate(I[0]):
        print(f"Ï≤≠ÌÅ¨{idx+1}: { chunks[idx]}")


    context_texts = [chunks[idx] for idx in I[0]]
    return "\n".join(context_texts)





trans_path = "./trans/ko-en"
tokenizer = MarianTokenizer.from_pretrained(trans_path)
model = MarianMTModel.from_pretrained(trans_path)

def translate(text):
    tokens = tokenizer(text, return_tensors="pt", padding=True)
    generated_ids = model.generate(**tokens)
    return tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)






def split_sentences_korean(text):
    sentences = re.split(r'(?<=[.!?])\s*', text)    # ÎßàÏπ®Ìëú, Î¨ºÏùåÌëú, ÎäêÎÇåÌëú Îí§ÏóêÏÑúÎßå Î¨∏Ïû• Î∂ÑÎ¶¨

    return [s.strip() for s in sentences if s.strip()]






























LLAMA_RUN_PATH = "D:\\model\\llama.cpp\\build\\bin\\Release\\llama-run.exe"
MODEL_PATH = "D:\\model\\llama-3-Korean-Bllossom-8B-Q4_K_M.gguf"

st.set_page_config(page_title="Ï±óÎ¥á", page_icon=":robot_face:")
st.title(":red[NHIS] :blue[_ÎùºÎßà3_] Ï±óÎ¥á :robot_face:")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

for role, msg in st.session_state["messages"]:
    st.chat_message(role).write(msg)

question = st.chat_input("ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî.")
if question:
    st.chat_message("user").write(question)

    translated = translate(question)
    context_text = retrieve_context(translated)   # RAG Ï≤≠ÌÅ¨




    # question_trans = f"Please answer between 80 and 150 tokens in Korean. {translated}\n\nÍ¥ÄÎ†® ÎÇ¥Ïö©:\n{context_text}"
    # question_trans = "Please answer between 80 and 150 tokens in Korean. " + translated


    context_sentences = split_sentences_korean(context_text)

    translated_context = "\n".join(translate(sent) for sent in context_sentences)

    question_trans = (
        f"Please answer between 80 and 150 tokens in Korean. {translated}\n\n"
        f"Í¥ÄÎ†® ÎÇ¥Ïö©:\n{translated_context}"
        )



    # print("Î≤àÏó≠Î¨∏:", question_trans)  # ÎîîÎ≤ÑÍ∑∏ Ï∂úÎ†•

    prompt = f"<s>[INST] {question_trans} [/INST]"

    ansi_escape = re.compile(r'\x1B[@-_][0-?]*[ -/]*[@-~]')

    process = subprocess.Popen(
        [LLAMA_RUN_PATH, MODEL_PATH, "-n", "256", "--temp", "0.7", "--top-p", "0.9", prompt],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        encoding='utf-8',
        bufsize=1,
    )

    message_area = st.chat_message("assistant").empty()
    # message_area.write("ÎãµÎ≥Ä ÏÉùÏÑ±Ï§ë...")

    answer = ""






    import collections

    ignore_tokens = ["[INST]", "[/INST]", "<INST>", "</INST>", "<|INST>", "<|INST|>", "</INST|>", "Loading model8", "<s>", "</s>", "[K", "[0m", "--top-p 0.9", "ÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ:"]
    token_buffer = ""
    visible_text = ""
    deque = collections.deque()
    max_token_len = max(len(t) for t in ignore_tokens)


    answer_session = ""

    while True:
        char = process.stdout.read(1)
        if char == "" and process.poll() is not None:
            break
        if not char:
            continue

        deque.append(char)
        token_buffer = ''.join(deque)

        # Í∏àÏßÄ ÌÜ†ÌÅ∞Ïù¥ Î∞úÍ≤¨ÎêòÎ©¥, Ìï¥Îãπ Í∏∏Ïù¥ÎßåÌÅº ÏÇ≠Ï†ú
        matched = False
        for token in ignore_tokens:
            if token_buffer.endswith(token):
                # Í∏àÏßÄÏñ¥Í∞Ä ÎÅùÏóê Îß§ÏπòÎêòÎ©¥ Í∑∏ÎßåÌÅº Ï†úÍ±∞
                for _ in range(len(token)):
                    deque.pop()
                matched = True
                break

        # ÌÜ†ÌÅ∞Ïù¥ ÏóÜÍ≥† deque Í∏∏Ïù¥Í∞Ä Ï∂©Î∂ÑÌïòÎ©¥ ÏïûÍ∏ÄÏûê Ï∂úÎ†•
        if not matched and len(deque) > max_token_len:
            visible_text += deque.popleft()
            clean = visible_text.replace("Î≥¥Í±¥Î≥¥Ìóò", "Í±¥Í∞ïÎ≥¥Ìóò").replace("Loading model", "").strip()
            message_area.write(clean)



    
    # ÎÇ®ÏùÄ Í∏ÄÏûê Ï∂úÎ†•
    while deque:
        visible_text += deque.popleft()
        clean = visible_text.replace("Î≥¥Í±¥Î≥¥Ìóò", "Í±¥Í∞ïÎ≥¥Ìóò").replace("Loading model", "").strip()
        message_area.write(clean)
    











    process.stdout.close()
    process.wait()

    st.session_state["messages"].append(("user", question))
    st.session_state["messages"].append(("assistant", clean))





















    
